{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-22T16:27:39.975822Z",
     "iopub.status.busy": "2025-01-22T16:27:39.975426Z",
     "iopub.status.idle": "2025-01-22T16:27:43.307405Z",
     "shell.execute_reply": "2025-01-22T16:27:43.306247Z",
     "shell.execute_reply.started": "2025-01-22T16:27:39.975781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboardX) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->tensorboardX) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:27:43.309413Z",
     "iopub.status.busy": "2025-01-22T16:27:43.309024Z",
     "iopub.status.idle": "2025-01-22T16:28:13.441847Z",
     "shell.execute_reply": "2025-01-22T16:28:13.440829Z",
     "shell.execute_reply.started": "2025-01-22T16:27:43.309378Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->tensorboardX) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboardX) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->tensorboardX) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->tensorboardX) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->tensorboardX) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->tensorboardX) (2024.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (11.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->scikit-learn) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->scikit-learn) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->scikit-learn) (2024.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2,>=1.20->matplotlib) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2,>=1.20->matplotlib) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2,>=1.20->matplotlib) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2,>=1.20->matplotlib) (2024.2.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.27.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.12.14)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Essential packages for deep learning and data processing\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install tensorboardX\n",
    "!pip install tqdm\n",
    "!pip install numpy\n",
    "!pip install Pillow  # for image processing\n",
    "!pip install scikit-learn  # for train-test splitting utilities\n",
    "\n",
    "# Additional utilities that might be needed\n",
    "!pip install matplotlib  # for visualization if needed\n",
    "!pip install einops  # often used with Vision Transformers\n",
    "!pip install timm  # useful for ViT implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:13.443739Z",
     "iopub.status.busy": "2025-01-22T16:28:13.443403Z",
     "iopub.status.idle": "2025-01-22T16:28:16.714900Z",
     "shell.execute_reply": "2025-01-22T16:28:16.714014Z",
     "shell.execute_reply.started": "2025-01-22T16:28:13.443713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections) (1.17.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ml-collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:16.717037Z",
     "iopub.status.busy": "2025-01-22T16:28:16.716803Z",
     "iopub.status.idle": "2025-01-22T16:28:16.725478Z",
     "shell.execute_reply": "2025-01-22T16:28:16.724628Z",
     "shell.execute_reply.started": "2025-01-22T16:28:16.717016Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cswin_unet.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cswin_unet.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from einops.layers.torch import Rearrange\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LePEAttention(nn.Module):\n",
    "    def __init__(self, dim, resolution, idx, split_size, dim_out=None, num_heads=9, attn_drop=0., proj_drop=0.,\n",
    "                 qk_scale=None):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dim_out = dim_out or dim\n",
    "        self.resolution = resolution\n",
    "        self.split_size = split_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        if idx == -1:\n",
    "            H_sp, W_sp = self.resolution, self.resolution\n",
    "        elif idx == 0:\n",
    "            H_sp, W_sp = self.resolution, self.split_size\n",
    "        elif idx == 1:\n",
    "            W_sp, H_sp = self.resolution, self.split_size\n",
    "        else:\n",
    "            print(\"ERROR MODE\", idx)\n",
    "            exit(0)\n",
    "        self.H_sp = H_sp\n",
    "        self.W_sp = W_sp\n",
    "        stride = 1\n",
    "        self.get_v = nn.Conv2d(dim, dim, kernel_size=3, stride=1, padding=1, groups=dim)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "    def im2cswin(self, x):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(np.sqrt(N))\n",
    "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
    "        x = img2windows(x, self.H_sp, self.W_sp)\n",
    "        x = x.reshape(-1, self.H_sp * self.W_sp, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3).contiguous()\n",
    "        return x\n",
    "\n",
    "    def get_lepe(self, x, func):\n",
    "        B, N, C = x.shape\n",
    "        H = W = int(np.sqrt(N))\n",
    "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
    "\n",
    "        H_sp, W_sp = self.H_sp, self.W_sp\n",
    "        x = x.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
    "        x = x.permute(0, 2, 4, 1, 3, 5).contiguous().reshape(-1, C, H_sp, W_sp)  ### B', C, H', W'\n",
    "\n",
    "        lepe = func(x)  ### B', C, H', W'\n",
    "        lepe = lepe.reshape(-1, self.num_heads, C // self.num_heads, H_sp * W_sp).permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        x = x.reshape(-1, self.num_heads, C // self.num_heads, self.H_sp * self.W_sp).permute(0, 1, 3, 2).contiguous()\n",
    "        return x, lepe\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        x: B L C\n",
    "        \"\"\"\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        ### Img2Window\n",
    "        H = W = self.resolution\n",
    "        B, L, C = q.shape\n",
    "\n",
    "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
    "\n",
    "        q = self.im2cswin(q)\n",
    "        k = self.im2cswin(k)\n",
    "        v, lepe = self.get_lepe(v, self.get_v)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))  # B head N C @ B head C N --> B head N N\n",
    "        attn = nn.functional.softmax(attn, dim=-1, dtype=attn.dtype)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v) + lepe\n",
    "        x = x.transpose(1, 2).reshape(-1, self.H_sp * self.W_sp, C)  # B head N N @ B head N C\n",
    "\n",
    "        ### Window2Img\n",
    "        x = windows2img(x, self.H_sp, self.W_sp, H, W).view(B, -1, C)  # B H' W' C\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSWinBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, reso, num_heads,\n",
    "                 split_size, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
    "                 drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n",
    "                 last_stage=False):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.patches_resolution = reso\n",
    "        self.split_size = split_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        if self.patches_resolution == split_size:\n",
    "            last_stage = True\n",
    "        if last_stage:\n",
    "            self.branch_num = 1\n",
    "        else:\n",
    "            self.branch_num = 2\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(drop)\n",
    "\n",
    "        if last_stage:\n",
    "            self.attns = nn.ModuleList([\n",
    "                LePEAttention(\n",
    "                    dim, resolution=self.patches_resolution, idx=-1,\n",
    "                    split_size=split_size, num_heads=num_heads, dim_out=dim,\n",
    "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "                for i in range(self.branch_num)])\n",
    "\n",
    "        else:\n",
    "            self.attns = nn.ModuleList([\n",
    "                LePEAttention(\n",
    "                    dim // 2, resolution=self.patches_resolution, idx=i,\n",
    "                    split_size=split_size, num_heads=num_heads // 2, dim_out=dim // 2,\n",
    "                    qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "                for i in range(self.branch_num)])\n",
    "\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer,\n",
    "                       drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "\n",
    "        H = W = self.patches_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"flatten img_tokens has wrong size\"\n",
    "        img = self.norm1(x)\n",
    "        qkv = self.qkv(img).reshape(B, -1, 3, C).permute(2, 0, 1, 3)\n",
    "\n",
    "        if self.branch_num == 2:\n",
    "            x1 = self.attns[0](qkv[:, :, :, :C // 2])\n",
    "            x2 = self.attns[1](qkv[:, :, :, C // 2:])\n",
    "            attened_x = torch.cat([x1, x2], dim=2)\n",
    "        else:\n",
    "            attened_x = self.attns[0](qkv)\n",
    "        attened_x = self.proj(attened_x)\n",
    "        x = x + self.drop_path(attened_x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def img2windows(img, H_sp, W_sp):\n",
    "    \"\"\"\n",
    "    img: B C H W\n",
    "    \"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    img_reshape = img.view(B, C, H // H_sp, H_sp, W // W_sp, W_sp)\n",
    "    img_perm = img_reshape.permute(0, 2, 4, 3, 5, 1).contiguous().reshape(-1, H_sp * W_sp, C)\n",
    "    return img_perm\n",
    "\n",
    "\n",
    "def windows2img(img_splits_hw, H_sp, W_sp, H, W):\n",
    "    \"\"\"\n",
    "    img_splits_hw: B' H W C\n",
    "    \"\"\"\n",
    "    B = int(img_splits_hw.shape[0] / (H * W / H_sp / W_sp))\n",
    "\n",
    "    img = img_splits_hw.view(B, H // H_sp, W // W_sp, H_sp, W_sp, -1)\n",
    "    img = img.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return img\n",
    "\n",
    "\n",
    "class Merge_Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(dim, dim_out, 3, 2, 1)\n",
    "        self.norm = norm_layer(dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, new_HW, C = x.shape\n",
    "        H = W = int(np.sqrt(new_HW))\n",
    "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
    "        x = self.conv(x)\n",
    "        B, C = x.shape[:2]\n",
    "        x = x.view(B, C, -1).transpose(-2, -1).contiguous()\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class CARAFE(nn.Module):\n",
    "    def __init__(self, dim, dim_out, kernel_size=3, up_factor=2):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_factor = up_factor\n",
    "        self.down = nn.Conv2d(dim, dim // 4, 1)\n",
    "        self.encoder = nn.Conv2d(dim // 4, self.up_factor ** 2 * self.kernel_size ** 2,\n",
    "                                 self.kernel_size, 1, self.kernel_size // 2)\n",
    "        self.out = nn.Conv2d(dim, dim_out, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, new_HW, C = x.shape\n",
    "        H = W = int(np.sqrt(new_HW))\n",
    "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
    "\n",
    "\n",
    "            # N,C,H,W -> N,C,delta*H,delta*W\n",
    "            # kernel prediction module\n",
    "        kernel_tensor = self.down(x)  # (N, Cm, H, W)\n",
    "        kernel_tensor = self.encoder(kernel_tensor)  # (N, S^2 * Kup^2, H, W)\n",
    "        kernel_tensor = F.pixel_shuffle(kernel_tensor,\n",
    "                                        self.up_factor)  # (N, S^2 * Kup^2, H, W)->(N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = F.softmax(kernel_tensor, dim=1)  # (N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = kernel_tensor.unfold(2, self.up_factor, step=self.up_factor)  # (N, Kup^2, H, W*S, S)\n",
    "        kernel_tensor = kernel_tensor.unfold(3, self.up_factor, step=self.up_factor)  # (N, Kup^2, H, W, S, S)\n",
    "        kernel_tensor = kernel_tensor.reshape(B, self.kernel_size ** 2, H, W,\n",
    "                                                  self.up_factor ** 2)  # (N, Kup^2, H, W, S^2)\n",
    "        kernel_tensor = kernel_tensor.permute(0, 2, 3, 1, 4)  # (N, H, W, Kup^2, S^2)\n",
    "\n",
    "            # content-aware reassembly module\n",
    "            # tensor.unfold: dim, size, step\n",
    "        w = F.pad(x, pad=(self.kernel_size // 2, self.kernel_size // 2,\n",
    "                                              self.kernel_size // 2, self.kernel_size // 2),\n",
    "                              mode='constant', value=0)  # (N, C, H+Kup//2+Kup//2, W+Kup//2+Kup//2)\n",
    "        w = w.unfold(2, self.kernel_size, step=1)  # (N, C, H, W+Kup//2+Kup//2, Kup)\n",
    "        w = w.unfold(3, self.kernel_size, step=1)  # (N, C, H, W, Kup, Kup)\n",
    "        w = w.reshape(B, C, H, W, -1)  # (N, C, H, W, Kup^2)\n",
    "        w = w.permute(0, 2, 3, 1, 4)  # (N, H, W, C, Kup^2)\n",
    "\n",
    "        x = torch.matmul(w, kernel_tensor)  # (N, H, W, C, S^2)\n",
    "        x = x.reshape(B, H, W, -1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pixel_shuffle(x, self.up_factor)\n",
    "        x = self.out(x)\n",
    "        B, C = x.shape[:2]\n",
    "        x = x.view(B, C, -1).transpose(-2, -1).contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CARAFE4(nn.Module):\n",
    "    def __init__(self, dim, dim_out, kernel_size=3, up_factor=4):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.up_factor = up_factor\n",
    "        self.down = nn.Conv2d(dim, dim // 4, 1)\n",
    "        self.encoder = nn.Conv2d(dim // 4, self.up_factor ** 2 * self.kernel_size ** 2,\n",
    "                                 self.kernel_size, 1, self.kernel_size // 2)\n",
    "        self.out = nn.Conv2d(dim, dim_out, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, new_HW, C = x.shape\n",
    "        H = W = int(np.sqrt(new_HW))\n",
    "        x = x.transpose(-2, -1).contiguous().view(B, C, H, W)\n",
    "\n",
    "\n",
    "            # N,C,H,W -> N,C,delta*H,delta*W\n",
    "            # kernel prediction module\n",
    "        kernel_tensor = self.down(x)  # (N, Cm, H, W)\n",
    "        kernel_tensor = self.encoder(kernel_tensor)  # (N, S^2 * Kup^2, H, W)\n",
    "        kernel_tensor = F.pixel_shuffle(kernel_tensor,\n",
    "                                        self.up_factor)  # (N, S^2 * Kup^2, H, W)->(N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = F.softmax(kernel_tensor, dim=1)  # (N, Kup^2, S*H, S*W)\n",
    "        kernel_tensor = kernel_tensor.unfold(2, self.up_factor, step=self.up_factor)  # (N, Kup^2, H, W*S, S)\n",
    "        kernel_tensor = kernel_tensor.unfold(3, self.up_factor, step=self.up_factor)  # (N, Kup^2, H, W, S, S)\n",
    "        kernel_tensor = kernel_tensor.reshape(B, self.kernel_size ** 2, H, W,\n",
    "                                                  self.up_factor ** 2)  # (N, Kup^2, H, W, S^2)\n",
    "        kernel_tensor = kernel_tensor.permute(0, 2, 3, 1, 4)  # (N, H, W, Kup^2, S^2)\n",
    "\n",
    "            # content-aware reassembly module\n",
    "            # tensor.unfold: dim, size, step\n",
    "        w = F.pad(x, pad=(self.kernel_size // 2, self.kernel_size // 2,\n",
    "                                              self.kernel_size // 2, self.kernel_size // 2),\n",
    "                              mode='constant', value=0)  # (N, C, H+Kup//2+Kup//2, W+Kup//2+Kup//2)\n",
    "        w = w.unfold(2, self.kernel_size, step=1)  # (N, C, H, W+Kup//2+Kup//2, Kup)\n",
    "        w = w.unfold(3, self.kernel_size, step=1)  # (N, C, H, W, Kup, Kup)\n",
    "        w = w.reshape(B, C, H, W, -1)  # (N, C, H, W, Kup^2)\n",
    "        w = w.permute(0, 2, 3, 1, 4)  # (N, H, W, C, Kup^2)\n",
    "\n",
    "        x = torch.matmul(w, kernel_tensor)  # (N, H, W, C, S^2)\n",
    "        x = x.reshape(B, H, W, -1)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pixel_shuffle(x, self.up_factor)\n",
    "        x = self.out(x)\n",
    "        B, C = x.shape[:2]\n",
    "        x = x.view(B, C, -1).transpose(-2, -1).contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CSWinTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=8, embed_dim=64, depth=[1, 2, 9, 1],\n",
    "                 split_size=[1, 2, 7, 7],\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0, hybrid_backbone=None, norm_layer=nn.LayerNorm, use_chk=False):\n",
    "        super().__init__()\n",
    "        self.use_chk = use_chk\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        heads = num_heads\n",
    "\n",
    "        #encoder\n",
    "\n",
    "        self.stage1_conv_embed = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, embed_dim, 7, 4, 2),\n",
    "            Rearrange('b c h w -> b (h w) c', h=img_size // 4, w=img_size // 4),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "\n",
    "        curr_dim = embed_dim\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, np.sum(depth))]  # stochastic depth decay rule\n",
    "        print(\"depth\",depth)\n",
    "        self.stage1 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[0], reso=img_size // 4, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[0],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth[0])])\n",
    "        self.merge1 = Merge_Block(curr_dim, curr_dim * 2)\n",
    "        curr_dim = curr_dim * 2\n",
    "        self.stage2 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[1], reso=img_size // 8, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[1],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:1]) + i], norm_layer=norm_layer)\n",
    "                for i in range(depth[1])])\n",
    "        self.merge2 = Merge_Block(curr_dim, curr_dim * 2)\n",
    "        curr_dim = curr_dim * 2\n",
    "        temp_stage3 = []\n",
    "        temp_stage3.extend(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[2], reso=img_size // 16, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[2],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:2]) + i], norm_layer=norm_layer)\n",
    "                for i in range(depth[2])])\n",
    "\n",
    "        self.stage3 = nn.ModuleList(temp_stage3)\n",
    "        self.merge3 = Merge_Block(curr_dim, curr_dim * 2)\n",
    "        curr_dim = curr_dim * 2\n",
    "        self.stage4 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[3], reso=img_size // 32, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[-1],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:-1]) + i], norm_layer=norm_layer, last_stage=True)\n",
    "                for i in range(depth[-1])])\n",
    "\n",
    "        self.norm = norm_layer(curr_dim)\n",
    "\n",
    "        # decoder\n",
    "\n",
    "\n",
    "        self.stage_up4 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[3], reso=img_size // 32, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[-1],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:-1]) + i], norm_layer=norm_layer, last_stage=True)\n",
    "                for i in range(depth[-1])])\n",
    "\n",
    "        self.upsample4 = CARAFE(curr_dim, curr_dim // 2)\n",
    "        curr_dim = curr_dim // 2\n",
    "\n",
    "        self.concat_linear4 = nn.Linear(512, 256)\n",
    "        self.stage_up3 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[2], reso=img_size // 16, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[2],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:2]) + i], norm_layer=norm_layer)\n",
    "                for i in range(depth[2])]\n",
    "        )\n",
    "\n",
    "        self.upsample3 = CARAFE(curr_dim, curr_dim // 2)\n",
    "        curr_dim = curr_dim // 2\n",
    "\n",
    "        self.concat_linear3 = nn.Linear(256, 128)\n",
    "        self.stage_up2 = nn.ModuleList(\n",
    "            [CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[1], reso=img_size // 8, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[1],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[np.sum(depth[:1]) + i], norm_layer=norm_layer)\n",
    "                for i in range(depth[1])])\n",
    "        self.upsample2 = CARAFE(curr_dim, curr_dim // 2)\n",
    "        curr_dim = curr_dim // 2\n",
    "\n",
    "        self.concat_linear2 = nn.Linear(128, 64)\n",
    "        self.stage_up1 = nn.ModuleList([\n",
    "            CSWinBlock(\n",
    "                dim=curr_dim, num_heads=heads[0], reso=img_size // 4, mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias, qk_scale=qk_scale, split_size=split_size[0],\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i], norm_layer=norm_layer)\n",
    "            for i in range(depth[0])])\n",
    "\n",
    "        self.upsample1 = CARAFE4(curr_dim, 64)\n",
    "        self.norm_up = norm_layer(embed_dim)\n",
    "        self.output = nn.Conv2d(in_channels=embed_dim, out_channels=self.num_classes, kernel_size=1, bias=False)\n",
    "        # Classifier head\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, (nn.LayerNorm, nn.BatchNorm2d)):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'relative_position_bias_table'}\n",
    "\n",
    "    #Encoder and Bottleneck\n",
    "    def forward_features(self, x):\n",
    "        x = self.stage1_conv_embed(x)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.stage1:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        self.x1 = x\n",
    "        x = self.merge1(x)\n",
    "\n",
    "        for blk in self.stage2:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        self.x2 = x\n",
    "        x = self.merge2(x)\n",
    "\n",
    "        for blk in self.stage3:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                    x = blk(x)\n",
    "        self.x3 = x\n",
    "        x = self.merge3(x)\n",
    "\n",
    "        for blk in self.stage4:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    #Dencoder and Skip connection\n",
    "    def forward_up_features(self, x):\n",
    "        for blk in self.stage_up4:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        x = self.upsample4(x)\n",
    "        x = torch.cat([self.x3, x],-1)\n",
    "        x = self.concat_linear4(x)\n",
    "        for blk in self.stage_up3:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        # print(\"decoder stage3\", x.shape)\n",
    "        x = self.upsample3(x)\n",
    "        x = torch.cat([self.x2, x],-1)\n",
    "        x = self.concat_linear3(x)\n",
    "        for blk in self.stage_up2:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                    x = blk(x)\n",
    "        x = self.upsample2(x)\n",
    "        x = torch.cat([self.x1, x],-1)\n",
    "        x = self.concat_linear2(x)\n",
    "        for blk in self.stage_up1:\n",
    "            if self.use_chk:\n",
    "                x = checkpoint.checkpoint(blk, x)\n",
    "            else:\n",
    "                x = blk(x)\n",
    "        x = self.norm_up(x)  # B L C\n",
    "        return x\n",
    "\n",
    "    def up_x4(self, x):\n",
    "        B, new_HW, C = x.shape\n",
    "        H = W = int(np.sqrt(new_HW))\n",
    "        x = self.upsample1(x)\n",
    "        x = x.view(B, 4 * H, 4 * W, -1)\n",
    "        x = x.permute(0, 3, 1, 2)  # B,C,H,W\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "\n",
    "        x = self.forward_up_features(x)\n",
    "\n",
    "        x = self.up_x4(x)\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:16.726743Z",
     "iopub.status.busy": "2025-01-22T16:28:16.726430Z",
     "iopub.status.idle": "2025-01-22T16:28:16.746419Z",
     "shell.execute_reply": "2025-01-22T16:28:16.745737Z",
     "shell.execute_reply.started": "2025-01-22T16:28:16.726711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vision_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile vision_transformer.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from cswin_unet import CSWinTransformer\n",
    "\n",
    "class CSwinUnet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=224,\n",
    "        num_classes=1,\n",
    "        patch_size=4,\n",
    "        embed_dim=64,\n",
    "        # embed_dim=96,\n",
    "        depth=[1, 2, 9, 1],\n",
    "        split_size=[1, 2, 7, 7],\n",
    "        num_heads=[2, 4, 8, 16],\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        pretrained_ckpt=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CSWinUnet model for deblurring.\n",
    "\n",
    "        :param img_size: Size of input images (default is 224).\n",
    "        :param num_classes: Number of output channels (1 for grayscale).\n",
    "        :param patch_size: Patch size for the transformer (default is 4).\n",
    "        :param embed_dim: Embedding dimension (default is 96).\n",
    "        :param depth: Depth of each stage (list of integers).\n",
    "        :param split_size: Split size for local attention at each stage.\n",
    "        :param num_heads: Number of attention heads at each stage.\n",
    "        :param mlp_ratio: MLP ratio in transformer blocks.\n",
    "        :param qkv_bias: Whether to use bias in QKV projection.\n",
    "        :param qk_scale: Scaling factor for QK projection (default is None).\n",
    "        :param drop_rate: Dropout rate.\n",
    "        :param drop_path_rate: Stochastic depth rate.\n",
    "        :param pretrained_ckpt: Path to a pretrained checkpoint file (optional).\n",
    "        \"\"\"\n",
    "        super(CSwinUnet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Initialize the CSWinTransformer model\n",
    "        self.cswin_unet = CSWinTransformer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=1,  # Grayscale images\n",
    "            num_classes=self.num_classes,\n",
    "            embed_dim=embed_dim,\n",
    "            depth=depth,\n",
    "            split_size=split_size,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop_rate=drop_rate,\n",
    "            drop_path_rate=drop_path_rate,\n",
    "        )\n",
    "\n",
    "        print(\"CSWinUnet model initialized.\")\n",
    "\n",
    "        # Load pretrained weights if provided\n",
    "        if pretrained_ckpt:\n",
    "            self.load_from(pretrained_ckpt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if x.size()[1] == 1:\n",
    "        #     x = x.repeat(1,3,1,1)\n",
    "        logits = self.cswin_unet(x)\n",
    "        return logits\n",
    "\n",
    "    def load_from(self, pretrained_ckpt):\n",
    "        \"\"\"\n",
    "        Load a pretrained model from a checkpoint file.\n",
    "\n",
    "        :param pretrained_ckpt: Path to the pretrained checkpoint file.\n",
    "        \"\"\"\n",
    "        print(f\"Loading pretrained model from {pretrained_ckpt}\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        pretrained_dict = torch.load(pretrained_ckpt, map_location=device)\n",
    "\n",
    "        # Ensure compatibility with current model structure\n",
    "        pretrained_dict = pretrained_dict.get('state_dict_ema', pretrained_dict)  # Adjust key if nested\n",
    "        model_dict = self.cswin_unet.state_dict()\n",
    "        full_dict = copy.deepcopy(pretrained_dict)\n",
    "\n",
    "        # Adjust stage keys for upsampling compatibility\n",
    "        for k, v in pretrained_dict.items():\n",
    "            if \"stage\" in k:\n",
    "                current_k = \"stage_up\" + k[5:]\n",
    "                full_dict[current_k] = v\n",
    "\n",
    "        # Remove mismatched keys\n",
    "        for k in list(full_dict.keys()):\n",
    "            if k in model_dict and full_dict[k].shape != model_dict[k].shape:\n",
    "                print(f\"Deleting key: {k}, shapes don't match (pretrained: {full_dict[k].shape}, model: {model_dict[k].shape})\")\n",
    "                del full_dict[k]\n",
    "\n",
    "        # Load state dict into the model\n",
    "        msg = self.cswin_unet.load_state_dict(full_dict, strict=False)\n",
    "        print(f\"Model loaded successfully: {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:16.747453Z",
     "iopub.status.busy": "2025-01-22T16:28:16.747184Z",
     "iopub.status.idle": "2025-01-22T16:28:16.764280Z",
     "shell.execute_reply": "2025-01-22T16:28:16.763544Z",
     "shell.execute_reply.started": "2025-01-22T16:28:16.747431Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class PairedXrayDataset(Dataset):\n",
    "    def __init__(self, blurred_dir, clear_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            blurred_dir (str): Directory with blurred images.\n",
    "            clear_dir (str): Directory with corresponding clear images.\n",
    "            transform (callable, optional): Optional transformations to apply to both blurred and clear images.\n",
    "        \"\"\"\n",
    "        # Verify directories exist\n",
    "        assert os.path.isdir(blurred_dir), f\"Directory not found: {blurred_dir}\"\n",
    "        assert os.path.isdir(clear_dir), f\"Directory not found: {clear_dir}\"\n",
    "        \n",
    "        self.blurred_dir = blurred_dir\n",
    "        self.clear_dir = clear_dir\n",
    "        self.transform = transform\n",
    "        self.blurred_images = sorted(os.listdir(blurred_dir))\n",
    "        self.clear_images = sorted(os.listdir(clear_dir))\n",
    "\n",
    "        # Ensure both directories have the same number of images\n",
    "        assert len(self.blurred_images) == len(self.clear_images), \\\n",
    "            \"Mismatched number of images in blurred and clear directories.\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.blurred_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get file paths for blurred and clear images\n",
    "        blurred_path = os.path.join(self.blurred_dir, self.blurred_images[idx])\n",
    "        clear_path = os.path.join(self.clear_dir, self.clear_images[idx])\n",
    "\n",
    "        # Load images and convert to grayscale if necessary\n",
    "        blurred_image = Image.open(blurred_path).convert(\"L\")  # \"L\" mode for grayscale\n",
    "        clear_image = Image.open(clear_path).convert(\"L\")\n",
    "\n",
    "        # Apply transformations if provided\n",
    "        if self.transform:\n",
    "            blurred_image = self.transform(blurred_image)\n",
    "            clear_image = self.transform(clear_image)\n",
    "        else:\n",
    "            # Default transformations: resize and convert to tensor\n",
    "            default_transform = transforms.Compose([\n",
    "                transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            blurred_image = default_transform(blurred_image)\n",
    "            clear_image = default_transform(clear_image)\n",
    "\n",
    "        return blurred_image, clear_image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:16.765208Z",
     "iopub.status.busy": "2025-01-22T16:28:16.764994Z",
     "iopub.status.idle": "2025-01-22T16:28:20.058201Z",
     "shell.execute_reply": "2025-01-22T16:28:20.057012Z",
     "shell.execute_reply.started": "2025-01-22T16:28:16.765189Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-msssim in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from pytorch-msssim) (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->pytorch-msssim) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->pytorch-msssim) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-msssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:28:20.061142Z",
     "iopub.status.busy": "2025-01-22T16:28:20.060870Z",
     "iopub.status.idle": "2025-01-22T16:28:20.071570Z",
     "shell.execute_reply": "2025-01-22T16:28:20.070767Z",
     "shell.execute_reply.started": "2025-01-22T16:28:20.061119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from pytorch_msssim import ssim\n",
    "\n",
    "# class CharbonnierLoss(nn.Module):\n",
    "#     \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "#     def __init__(self, eps=1e-6, out_norm='bci'):\n",
    "#         super(CharbonnierLoss, self).__init__()\n",
    "#         self.eps = eps\n",
    "#         self.out_norm = out_norm\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         def get_outnorm(tensor: torch.Tensor, out_norm: str):\n",
    "#             img_shape = tensor.shape\n",
    "#             norm = 1\n",
    "#             if 'b' in out_norm:\n",
    "#                 norm /= img_shape[0]  # normalize by batch size\n",
    "#             if 'c' in out_norm:\n",
    "#                 norm /= img_shape[-3]  # normalize by channels\n",
    "#             if 'i' in out_norm:\n",
    "#                 norm /= img_shape[-1] * img_shape[-2]  # normalize by image size\n",
    "#             return norm\n",
    "\n",
    "#         norm = get_outnorm(x, self.out_norm)\n",
    "#         loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
    "#         return loss * norm\n",
    "\n",
    "# class PerceptualLoss(nn.Module):\n",
    "#     \"\"\"Perceptual Loss using VGG features\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(PerceptualLoss, self).__init__()\n",
    "#         vgg = models.vgg19(pretrained=True).features.eval()\n",
    "#         self.layers = [0, 5, 10, 19, 28]  # Conv layers for perceptual features\n",
    "#         self.vgg_layers = nn.ModuleList([vgg[i] for i in self.layers])\n",
    "#         for param in self.vgg_layers.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def forward(self, generated, target):\n",
    "#         loss = 0.0\n",
    "#         x_gen, x_target = generated, target\n",
    "#         for layer in self.vgg_layers:\n",
    "#             x_gen, x_target = layer(x_gen), layer(x_target)\n",
    "#             loss += nn.functional.l1_loss(x_gen, x_target)\n",
    "#         return loss\n",
    "\n",
    "# # Combined Loss with Charbonnier, SSIM, and Perceptual Loss\n",
    "# class CombinedLoss(nn.Module):\n",
    "#     def __init__(self, perceptual_model, charbonnier_eps=1e-6, perceptual_weight=0.1, ssim_weight=0.1):\n",
    "#         super(CombinedLoss, self).__init__()\n",
    "#         self.charbonnier_loss = CharbonnierLoss(eps=charbonnier_eps)\n",
    "#         self.perceptual_model = perceptual_model\n",
    "#         self.perceptual_model.eval()  # Set VGG to evaluation mode\n",
    "#         for param in self.perceptual_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         self.perceptual_weight = perceptual_weight\n",
    "#         self.ssim_weight = ssim_weight\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         # Ensure 3 channels for perceptual loss\n",
    "#         if output.size(1) == 1:  # Check if input has 1 channel\n",
    "#             output = output.repeat(1, 3, 1, 1)  # Duplicate channels\n",
    "#         if target.size(1) == 1:\n",
    "#             target = target.repeat(1, 3, 1, 1)\n",
    "\n",
    "#         # Charbonnier loss\n",
    "#         l_charbonnier = self.charbonnier_loss(output, target)\n",
    "\n",
    "#         # SSIM loss\n",
    "#         l_ssim = 1 - ssim(output, target)\n",
    "\n",
    "#         # Perceptual loss\n",
    "#         output_features = self.perceptual_model(output)\n",
    "#         target_features = self.perceptual_model(target)\n",
    "#         l_perceptual = torch.mean((output_features - target_features).pow(2))\n",
    "\n",
    "#         # Weighted combined loss\n",
    "#         loss = l_charbonnier + self.ssim_weight * l_ssim + self.perceptual_weight * l_perceptual\n",
    "#         return loss\n",
    "\n",
    "\n",
    "# class MAELoss(nn.Module):\n",
    "#     \"\"\"Mean Absolute Error Loss\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super(MAELoss, self).__init__()\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         return torch.mean(torch.abs(x - y))\n",
    "\n",
    "\n",
    "# class CombinedLoss(nn.Module):\n",
    "#     def __init__(self, mae_weight=1.0, ssim_weight=0.1):\n",
    "#         super(CombinedLoss, self).__init__()\n",
    "#         self.mae_loss = MAELoss()\n",
    "#         self.ssim_weight = ssim_weight\n",
    "\n",
    "#     def forward(self, output, target):\n",
    "#         # Ensure the input images are grayscale (single channel), no need to repeat channels for SSIM\n",
    "#         if output.size(1) == 1 and target.size(1) == 1:\n",
    "#             # MAE loss (pixel-wise difference)\n",
    "#             l_mae = self.mae_loss(output, target)\n",
    "\n",
    "#             # SSIM loss (1 - SSIM to make it a loss function)\n",
    "#             l_ssim = 1 - ssim(output, target)\n",
    "\n",
    "#         else:\n",
    "#             raise ValueError(\"Both output and target should have 1 channel (grayscale images).\")\n",
    "\n",
    "#         # Weighted combined loss\n",
    "#         loss = l_mae + self.ssim_weight * l_ssim\n",
    "#         return loss\n",
    "\n",
    "\n",
    "def trainer_synapse(args, model, snapshot_path, dataloader):\n",
    "    \"\"\"\n",
    "    Training function for deblurring model using Combined loss.\n",
    "    Args:\n",
    "        args: Command-line arguments.\n",
    "        model: The initialized model (e.g., TransUNet).\n",
    "        snapshot_path: Path to save model checkpoints.\n",
    "        dataloader: DataLoader providing training batches of blurred and clear images.\n",
    "    \"\"\"\n",
    "    # Create snapshot directory if it doesn't exist\n",
    "    os.makedirs(snapshot_path, exist_ok=True)\n",
    "\n",
    "    # Set up Combined loss, optimizer, and learning rate scheduler\n",
    "    combined_loss = CombinedLoss(lambda_ssim=args.lambda_ssim, lambda_perceptual=args.lambda_perceptual)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.base_lr, betas=(0.9, 0.999), weight_decay=0.0001)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.max_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Enable GPU support if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(args.max_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}/{args.max_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (blurred_image, clear_image) in enumerate(dataloader):\n",
    "                blurred_image = blurred_image.to(device)\n",
    "                clear_image = clear_image.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(blurred_image)\n",
    "\n",
    "                # Calculate losses\n",
    "                loss = combined_loss(output, clear_image)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update running loss\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': running_loss / (i + 1)})\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Step the scheduler at the end of each epoch\n",
    "            scheduler.step()\n",
    "\n",
    "        # Save the model checkpoint every few epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(snapshot_path, f\"epoch_{epoch+1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Model checkpoint saved at {checkpoint_path}\")\n",
    "\n",
    "    # Final save after training\n",
    "    final_model_path = os.path.join(snapshot_path, \"final_model.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    print(f\"Final model saved at {final_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T16:36:58.387404Z",
     "iopub.status.busy": "2025-01-22T16:36:58.387018Z",
     "iopub.status.idle": "2025-01-22T18:11:09.973371Z",
     "shell.execute_reply": "2025-01-22T18:11:09.972422Z",
     "shell.execute_reply.started": "2025-01-22T16:36:58.387364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "depth [1, 2, 9, 1]\n",
      "CSWinUnet model initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-fcc69352334c>:146: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "Epoch 1/150:   0%|          | 0/160 [00:00<?, ?batch/s]<ipython-input-21-fcc69352334c>:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0905]\n",
      "Epoch 2/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0404]\n",
      "Epoch 3/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0374]\n",
      "Epoch 4/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0326]\n",
      "Epoch 5/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0343]\n",
      "Epoch 6/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0332]\n",
      "Epoch 7/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0307]\n",
      "Epoch 8/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0295]\n",
      "Epoch 9/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0298]\n",
      "Epoch 10/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/150: 100%|| 160/160 [00:38<00:00,  4.17batch/s, Loss=0.0287]\n",
      "Epoch 12/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.028] \n",
      "Epoch 13/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0275]\n",
      "Epoch 14/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.0271]\n",
      "Epoch 15/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.0263]\n",
      "Epoch 16/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.0272]\n",
      "Epoch 17/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0247]\n",
      "Epoch 18/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0266]\n",
      "Epoch 19/150: 100%|| 160/160 [00:38<00:00,  4.20batch/s, Loss=0.0242]\n",
      "Epoch 20/150: 100%|| 160/160 [00:38<00:00,  4.18batch/s, Loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/150: 100%|| 160/160 [00:38<00:00,  4.18batch/s, Loss=0.0261]\n",
      "Epoch 22/150: 100%|| 160/160 [00:38<00:00,  4.20batch/s, Loss=0.0231]\n",
      "Epoch 23/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0236]\n",
      "Epoch 24/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0249]\n",
      "Epoch 25/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0249]\n",
      "Epoch 26/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.0233]\n",
      "Epoch 27/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0233]\n",
      "Epoch 28/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.022] \n",
      "Epoch 29/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.021] \n",
      "Epoch 30/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/150: 100%|| 160/160 [00:38<00:00,  4.20batch/s, Loss=0.0216]\n",
      "Epoch 32/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0218]\n",
      "Epoch 33/150: 100%|| 160/160 [00:37<00:00,  4.21batch/s, Loss=0.0205]\n",
      "Epoch 34/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0219]\n",
      "Epoch 35/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0212]\n",
      "Epoch 36/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0203]\n",
      "Epoch 37/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0202]\n",
      "Epoch 38/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0198]\n",
      "Epoch 39/150: 100%|| 160/160 [00:38<00:00,  4.21batch/s, Loss=0.02]  \n",
      "Epoch 40/150: 100%|| 160/160 [00:38<00:00,  4.21batch/s, Loss=0.0186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/150: 100%|| 160/160 [00:38<00:00,  4.13batch/s, Loss=0.0182]\n",
      "Epoch 42/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0192]\n",
      "Epoch 43/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0183]\n",
      "Epoch 44/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0187]\n",
      "Epoch 45/150: 100%|| 160/160 [00:38<00:00,  4.20batch/s, Loss=0.0183]\n",
      "Epoch 46/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0176]\n",
      "Epoch 47/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.017] \n",
      "Epoch 48/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.017] \n",
      "Epoch 49/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.0165]\n",
      "Epoch 50/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0165]\n",
      "Epoch 52/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0172]\n",
      "Epoch 53/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0174]\n",
      "Epoch 54/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0159]\n",
      "Epoch 55/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0169]\n",
      "Epoch 56/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0161]\n",
      "Epoch 57/150: 100%|| 160/160 [00:38<00:00,  4.15batch/s, Loss=0.0156]\n",
      "Epoch 58/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0156]\n",
      "Epoch 59/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0156]\n",
      "Epoch 60/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0157]\n",
      "Epoch 62/150: 100%|| 160/160 [00:38<00:00,  4.20batch/s, Loss=0.0145]\n",
      "Epoch 63/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0159]\n",
      "Epoch 64/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0147]\n",
      "Epoch 65/150: 100%|| 160/160 [00:38<00:00,  4.21batch/s, Loss=0.0145]\n",
      "Epoch 66/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.014] \n",
      "Epoch 67/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0145]\n",
      "Epoch 68/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0146]\n",
      "Epoch 69/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0136]\n",
      "Epoch 70/150: 100%|| 160/160 [00:38<00:00,  4.17batch/s, Loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_70.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0138]\n",
      "Epoch 72/150: 100%|| 160/160 [00:37<00:00,  4.21batch/s, Loss=0.0134]\n",
      "Epoch 73/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0134]\n",
      "Epoch 74/150: 100%|| 160/160 [00:38<00:00,  4.18batch/s, Loss=0.013] \n",
      "Epoch 75/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0131]\n",
      "Epoch 76/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.0135]\n",
      "Epoch 77/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0128]\n",
      "Epoch 78/150: 100%|| 160/160 [00:37<00:00,  4.21batch/s, Loss=0.0126]\n",
      "Epoch 79/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0129]\n",
      "Epoch 80/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_80.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.0122]\n",
      "Epoch 82/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0121]\n",
      "Epoch 83/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0121]\n",
      "Epoch 84/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.0119]\n",
      "Epoch 85/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.0119]\n",
      "Epoch 86/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0118]\n",
      "Epoch 87/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0117]\n",
      "Epoch 88/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0118]\n",
      "Epoch 89/150: 100%|| 160/160 [00:37<00:00,  4.24batch/s, Loss=0.0116]\n",
      "Epoch 90/150: 100%|| 160/160 [00:37<00:00,  4.22batch/s, Loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_90.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/150: 100%|| 160/160 [00:38<00:00,  4.16batch/s, Loss=0.0115]\n",
      "Epoch 92/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0113]\n",
      "Epoch 93/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.0112]\n",
      "Epoch 94/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0114]\n",
      "Epoch 95/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0111]\n",
      "Epoch 96/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0109]\n",
      "Epoch 97/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0109]\n",
      "Epoch 98/150: 100%|| 160/160 [00:37<00:00,  4.31batch/s, Loss=0.0108]\n",
      "Epoch 99/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.0107]\n",
      "Epoch 100/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0107]\n",
      "Epoch 102/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0105]\n",
      "Epoch 103/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0104]\n",
      "Epoch 104/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.0105]\n",
      "Epoch 105/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.0104]\n",
      "Epoch 106/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.0103]\n",
      "Epoch 107/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0103]\n",
      "Epoch 108/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.0103]\n",
      "Epoch 109/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0103]\n",
      "Epoch 110/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.0101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_110.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0102]\n",
      "Epoch 112/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0101] \n",
      "Epoch 113/150: 100%|| 160/160 [00:37<00:00,  4.31batch/s, Loss=0.0101] \n",
      "Epoch 114/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.01]  \n",
      "Epoch 115/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00994]\n",
      "Epoch 116/150: 100%|| 160/160 [00:37<00:00,  4.31batch/s, Loss=0.00992]\n",
      "Epoch 117/150: 100%|| 160/160 [00:37<00:00,  4.31batch/s, Loss=0.00987]\n",
      "Epoch 118/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00985]\n",
      "Epoch 119/150: 100%|| 160/160 [00:37<00:00,  4.31batch/s, Loss=0.00983]\n",
      "Epoch 120/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.00981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_120.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00976]\n",
      "Epoch 122/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00978]\n",
      "Epoch 123/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00975]\n",
      "Epoch 124/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00971]\n",
      "Epoch 125/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00967]\n",
      "Epoch 126/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00967]\n",
      "Epoch 127/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00967]\n",
      "Epoch 128/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00963]\n",
      "Epoch 129/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00959]\n",
      "Epoch 130/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/150: 100%|| 160/160 [00:38<00:00,  4.19batch/s, Loss=0.00956]\n",
      "Epoch 132/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.00955]\n",
      "Epoch 133/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.00957]\n",
      "Epoch 134/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00953]\n",
      "Epoch 135/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00954]\n",
      "Epoch 136/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.0095] \n",
      "Epoch 137/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.00948]\n",
      "Epoch 138/150: 100%|| 160/160 [00:37<00:00,  4.25batch/s, Loss=0.0095] \n",
      "Epoch 139/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.0095] \n",
      "Epoch 140/150: 100%|| 160/160 [00:37<00:00,  4.23batch/s, Loss=0.00948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_140.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.00949]\n",
      "Epoch 142/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00945]\n",
      "Epoch 143/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00945]\n",
      "Epoch 144/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00947]\n",
      "Epoch 145/150: 100%|| 160/160 [00:37<00:00,  4.30batch/s, Loss=0.00946]\n",
      "Epoch 146/150: 100%|| 160/160 [00:37<00:00,  4.28batch/s, Loss=0.00945]\n",
      "Epoch 147/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00947]\n",
      "Epoch 148/150: 100%|| 160/160 [00:37<00:00,  4.29batch/s, Loss=0.00944]\n",
      "Epoch 149/150: 100%|| 160/160 [00:37<00:00,  4.27batch/s, Loss=0.00943]\n",
      "Epoch 150/150: 100%|| 160/160 [00:37<00:00,  4.26batch/s, Loss=0.00943]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint saved: /kaggle/working/model_checkpoints/epoch_150.pth\n",
      "\n",
      "Training completed. Model saved to /kaggle/working/model_checkpoints/final_model.pth\n",
      "Best loss: 0.009431 at epoch 150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from vision_transformer import CSwinUnet\n",
    "# from PairedXrayimages import PairedXrayDataset\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from pytorch_msssim import ssim\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import vgg19\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Data directories\n",
    "        self.clear_dir = '/kaggle/input/ch-xrays/augmented/augmented'\n",
    "        self.blurred_dir = '/kaggle/input/ch-xrays/aug_blurred/aug_blurred'\n",
    "        \n",
    "        # Model configuration\n",
    "        self.num_classes = 1\n",
    "        self.img_size = 224\n",
    "        \n",
    "        # Training parameters\n",
    "        self.max_epochs = 150\n",
    "        self.batch_size = 8\n",
    "        self.base_lr = 0.0005\n",
    "        self.deterministic = 1\n",
    "        self.seed = 1234\n",
    "        self.ssim_weight = 0.5\n",
    "        self.perceptual_weight = 0.1\n",
    "        \n",
    "        # Output paths\n",
    "        self.snapshot_path = '/kaggle/working/model_checkpoints'\n",
    "        self.plots_dir = '/kaggle/working/training_plots'\n",
    "\n",
    "# # Define combined loss\n",
    "# class CombinedLoss:\n",
    "#     def __init__(self, mae_weight=0.4, ssim_weight=0.6):\n",
    "#         self.mae_weight = mae_weight\n",
    "#         self.ssim_weight = ssim_weight\n",
    "#         self.mae_loss = nn.L1Loss()\n",
    "\n",
    "#     def __call__(self, output, target):\n",
    "#         mae = self.mae_loss(output, target)\n",
    "#         ssim_loss = 1 - ssim(output, target, data_range=1.0, size_average=True)\n",
    "#         return self.mae_weight * mae + self.ssim_weight * ssim_loss\n",
    "\n",
    "\n",
    "class CharbonnierLoss(nn.Module):\n",
    "    \"\"\"Charbonnier Loss (L1)\"\"\"\n",
    "    def __init__(self, eps=1e-6, out_norm='bci'):\n",
    "        super(CharbonnierLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.out_norm = out_norm\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        def get_outnorm(tensor: torch.Tensor, out_norm: str):\n",
    "            img_shape = tensor.shape\n",
    "            norm = 1\n",
    "            if 'b' in out_norm:\n",
    "                norm /= img_shape[0]  # normalize by batch size\n",
    "            if 'c' in out_norm:\n",
    "                norm /= img_shape[-3]  # normalize by channels\n",
    "            if 'i' in out_norm:\n",
    "                norm /= img_shape[-1] * img_shape[-2]  # normalize by image size\n",
    "            return norm\n",
    "\n",
    "        norm = get_outnorm(x, self.out_norm)\n",
    "        loss = torch.sum(torch.sqrt((x - y).pow(2) + self.eps**2))\n",
    "        return loss * norm\n",
    "\n",
    "class PerceptualLoss(nn.Module):\n",
    "    \"\"\"Perceptual Loss using VGG features\"\"\"\n",
    "    def __init__(self):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        vgg = models.vgg19(pretrained=True).features.eval()\n",
    "        self.layers = [0, 5, 10, 19, 28]  # Conv layers for perceptual features\n",
    "        self.vgg_layers = nn.ModuleList([vgg[i] for i in self.layers])\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, generated, target):\n",
    "        loss = 0.0\n",
    "        x_gen, x_target = generated, target\n",
    "        for layer in self.vgg_layers:\n",
    "            x_gen, x_target = layer(x_gen), layer(x_target)\n",
    "            loss += nn.functional.l1_loss(x_gen, x_target)\n",
    "        return loss\n",
    "\n",
    "# Combined Loss with Charbonnier, SSIM, and Perceptual Loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, perceptual_model, charbonnier_eps=1e-6, perceptual_weight=0.1, ssim_weight=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.charbonnier_loss = CharbonnierLoss(eps=charbonnier_eps)\n",
    "        self.perceptual_model = perceptual_model\n",
    "        self.perceptual_model.eval()  # Set VGG to evaluation mode\n",
    "        for param in self.perceptual_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.perceptual_weight = perceptual_weight\n",
    "        self.ssim_weight = ssim_weight\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # Ensure 3 channels for perceptual loss\n",
    "        if output.size(1) == 1:  # Check if input has 1 channel\n",
    "            output = output.repeat(1, 3, 1, 1)  # Duplicate channels\n",
    "        if target.size(1) == 1:\n",
    "            target = target.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Charbonnier loss\n",
    "        l_charbonnier = self.charbonnier_loss(output, target)\n",
    "\n",
    "        # SSIM loss\n",
    "        l_ssim = 1 - ssim(output, target)\n",
    "\n",
    "        # Perceptual loss\n",
    "        output_features = self.perceptual_model(output)\n",
    "        target_features = self.perceptual_model(target)\n",
    "        l_perceptual = torch.mean((output_features - target_features).pow(2))\n",
    "\n",
    "        # Weighted combined loss\n",
    "        loss = l_charbonnier + self.ssim_weight * l_ssim + self.perceptual_weight * l_perceptual\n",
    "        return loss\n",
    "\n",
    "def save_loss_plot(losses, plots_dir):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(losses) + 1), losses, 'b-')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(plots_dir, 'loss_plot.png'))\n",
    "    plt.close()\n",
    "\n",
    "def train_model(args, model, dataloader, optimizer, scheduler, combined_loss, device):\n",
    "    model.train()\n",
    "    scaler = GradScaler()\n",
    "    writer = SummaryWriter(logdir=args.snapshot_path)\n",
    "    \n",
    "    # Lists to store losses\n",
    "    epoch_losses = []\n",
    "    # mae_weight = 0.4\n",
    "    # ssim_weight = 0.6\n",
    "    # combined_loss = CombinedLoss(mae_weight, ssim_weight)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = 20\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Create directory for plots\n",
    "    os.makedirs(args.plots_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(args.max_epochs):\n",
    "        running_loss = 0.0\n",
    "        with tqdm(total=len(dataloader), desc=f\"Epoch {epoch+1}/{args.max_epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, (blurred, clear) in enumerate(dataloader):\n",
    "                blurred, clear = blurred.to(device), clear.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with autocast():\n",
    "                    output = model(blurred)\n",
    "                    loss = combined_loss(output, clear)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                pbar.set_postfix({'Loss': running_loss / (i + 1)})\n",
    "                pbar.update(1)\n",
    "\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Calculate and store average epoch loss\n",
    "            avg_epoch_loss = running_loss / len(dataloader)\n",
    "            epoch_losses.append(avg_epoch_loss)\n",
    "\n",
    "            # # Dynamic weight adjustment\n",
    "            # if epoch % 10 == 0 and epoch > 0:\n",
    "            #     if epoch_losses[-1] < best_loss:\n",
    "            #         if avg_epoch_loss - best_loss > 0.01:  # SSIM improving but PSNR poor\n",
    "            #             mae_weight = min(1.0, mae_weight + 0.05)\n",
    "            #         elif best_loss - avg_epoch_loss > 0.01:  # PSNR improving but SSIM stagnant\n",
    "            #             ssim_weight = min(1.0, ssim_weight + 0.05)\n",
    "            #         combined_loss = CombinedLoss(mae_weight, ssim_weight)\n",
    "\n",
    "            # Save epoch losses and plot\n",
    "            df_epochs = pd.DataFrame({\n",
    "                'Epoch': range(1, len(epoch_losses) + 1),\n",
    "                'Loss': epoch_losses\n",
    "            })\n",
    "            df_epochs.to_csv(os.path.join(args.plots_dir, 'epoch_losses.csv'), index=False)\n",
    "            save_loss_plot(epoch_losses, args.plots_dir)\n",
    "\n",
    "            writer.add_scalar('Training Loss', avg_epoch_loss, epoch)\n",
    "\n",
    "            # Early stopping check\n",
    "            if avg_epoch_loss < best_loss:\n",
    "                best_loss = avg_epoch_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}.\")\n",
    "                break\n",
    "\n",
    "            # Save checkpoint with only model_state_dict\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                checkpoint_path = os.path.join(args.snapshot_path, f\"epoch_{epoch+1}.pth\")\n",
    "                torch.save(model.state_dict(), checkpoint_path)\n",
    "                print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
    "\n",
    "    # Save final model with only model_state_dict\n",
    "    final_model_path = os.path.join(args.snapshot_path, \"final_model.pth\")\n",
    "    torch.save(model.state_dict(), final_model_path)\n",
    "    \n",
    "    print(f\"\\nTraining completed. Model saved to {final_model_path}\")\n",
    "    print(f\"Best loss: {min(epoch_losses):.6f} at epoch {epoch_losses.index(min(epoch_losses)) + 1}\")\n",
    "\n",
    "def main():\n",
    "    args = Args()\n",
    "\n",
    "    # Set deterministic behavior\n",
    "    if args.deterministic:\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.deterministic = True\n",
    "        random.seed(args.seed)\n",
    "        np.random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # Create snapshot directory\n",
    "    os.makedirs(args.snapshot_path, exist_ok=True)\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(\n",
    "        filename=os.path.join(args.snapshot_path, \"train.log\"),\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "    logging.info(\"Training started with configuration: %s\", vars(args))\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize model\n",
    "    model = CSwinUnet(img_size=args.img_size, num_classes=args.num_classes).to(device)\n",
    "    logging.info(\"Model initialized\")\n",
    "\n",
    "    # Initialize VGG for perceptual loss\n",
    "    vgg = vgg19(pretrained=True).features[:9].to(device).eval()\n",
    "    for param in vgg.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Initialize combined loss\n",
    "    combined_loss = CombinedLoss(\n",
    "        perceptual_model=vgg,\n",
    "        ssim_weight=args.ssim_weight,\n",
    "        perceptual_weight=args.perceptual_weight\n",
    "    )\n",
    "    logging.info(\"Loss functions initialized\")\n",
    "\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=args.base_lr)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=args.max_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((args.img_size, args.img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = PairedXrayDataset(\n",
    "        blurred_dir=args.blurred_dir,\n",
    "        clear_dir=args.clear_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    logging.info(f\"Dataloader initialized with {len(dataset)} samples\")\n",
    "\n",
    "    # Start training\n",
    "    train_model(args, model, dataloader, optimizer, scheduler, combined_loss, device)\n",
    "    logging.info(\"Training completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T18:30:29.814284Z",
     "iopub.status.busy": "2025-01-22T18:30:29.813905Z",
     "iopub.status.idle": "2025-01-22T18:31:15.397217Z",
     "shell.execute_reply": "2025-01-22T18:31:15.396405Z",
     "shell.execute_reply.started": "2025-01-22T18:30:29.814255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "depth [1, 2, 9, 1]\n",
      "CSWinUnet model initialized.\n",
      "Model loaded successfully\n",
      "Testing on 100 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-9b06f4144ddc>:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(args.model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 258_img_258_original_person496_bacteria_2095.jpeg_2e6bf10d-9f00-44c3-a8c4-3706a9b7113b.PNG | PSNR: 33.2562 | SSIM: 0.9246\n",
      "Processed: 259_img_259_original_person498_bacteria_2100.jpeg_30d2d192-3672-4e3f-8de1-fd8b4f03b152.PNG | PSNR: 33.9725 | SSIM: 0.9286\n",
      "Processed: 271_img_271_original_person707_virus_1305.jpeg_4e02b1ce-0e13-4f46-82bc-b551226f6ea9.PNG | PSNR: 34.6335 | SSIM: 0.9437\n",
      "Processed: 264_img_264_original_person593_bacteria_2435.jpeg_14025f99-7553-4fa8-818d-c4af7614859a.PNG | PSNR: 37.7017 | SSIM: 0.9526\n",
      "Processed: 275_img_275_original_person802_bacteria_2708.jpeg_ed48aec4-f0b2-4d13-a64b-bf99aebe2e9b.PNG | PSNR: 33.3045 | SSIM: 0.9248\n",
      "Processed: 257_img_257_original_person469_virus_965.jpeg_5d7a5d2c-87a1-4765-acc8-d365b68ffaee.PNG | PSNR: 35.8902 | SSIM: 0.9033\n",
      "Processed: 264_img_264_original_person593_bacteria_2435.jpeg_92ac4fa2-2769-4226-b9ea-0a73b24b4c40.PNG | PSNR: 37.4689 | SSIM: 0.9508\n",
      "Processed: 265_img_265_original_person609_virus_1176.jpeg_c3eb4017-782e-4662-94a3-7e34ce7ccfca.PNG | PSNR: 33.1987 | SSIM: 0.9258\n",
      "Processed: 268_img_268_original_person640_bacteria_2532.jpeg_92d39fbc-97f5-4004-bf3e-26f64671888b.PNG | PSNR: 35.4465 | SSIM: 0.9467\n",
      "Processed: 261_img_261_original_person527_virus_1048.jpeg_85dbdaf6-bf30-4e0e-a2f7-be7b67a74ea7.PNG | PSNR: 34.1519 | SSIM: 0.9347\n",
      "Processed: 272_img_272_original_person762_virus_1390.jpeg_18f3d1b1-10ac-4de6-b1da-b9b0fff63273.PNG | PSNR: 35.8898 | SSIM: 0.9456\n",
      "Processed: 273_img_273_original_person776_virus_1405.jpeg_be071a08-4bd6-453d-8ce8-91da4c0ddd2a.PNG | PSNR: 35.3353 | SSIM: 0.9520\n",
      "Processed: 259_img_259_original_person498_bacteria_2100.jpeg_cc9e2690-39cb-4ee4-8e6d-1a15eaeecc03.PNG | PSNR: 33.6950 | SSIM: 0.9295\n",
      "Processed: 274_img_274_original_person781_virus_1412.jpeg_6c6e4efc-6adc-4243-8edf-abfc06792689.PNG | PSNR: 32.7882 | SSIM: 0.9136\n",
      "Processed: 272_img_272_original_person762_virus_1390.jpeg_ec8b27c5-4b3f-4bdd-8167-664d05d521d9.PNG | PSNR: 35.8898 | SSIM: 0.9456\n",
      "Processed: 261_img_261_original_person527_virus_1048.jpeg_1e5acda9-c622-4edc-99ec-c5fece6effd5.PNG | PSNR: 34.1993 | SSIM: 0.9366\n",
      "Processed: 269_img_269_original_person641_bacteria_2533.jpeg_718418a2-04c3-4ccf-a4af-4d0a44f233f1.PNG | PSNR: 34.7967 | SSIM: 0.9427\n",
      "Processed: 272_img_272_original_person762_virus_1390.jpeg_a82af338-7d04-453a-afef-b71f16014673.PNG | PSNR: 35.7317 | SSIM: 0.9433\n",
      "Processed: 263_img_263_original_person550_bacteria_2309.jpeg_026fdbff-b587-478a-b11b-8ff3b8fa4329.PNG | PSNR: 33.9369 | SSIM: 0.9328\n",
      "Processed: 274_img_274_original_person781_virus_1412.jpeg_28e6514c-1666-483b-9f2b-0eb5702f5187.PNG | PSNR: 32.6148 | SSIM: 0.9126\n",
      "Processed: 269_img_269_original_person641_bacteria_2533.jpeg_f2795654-53cb-48f0-8fea-e961fbb1a09e.PNG | PSNR: 34.7967 | SSIM: 0.9427\n",
      "Processed: 276_img_276_original_person820_bacteria_2731.jpeg_f130f303-3486-49e8-a7f7-dcb5b29742b4.PNG | PSNR: 35.9559 | SSIM: 0.9420\n",
      "Processed: 266_img_266_original_person617_bacteria_2488.jpeg_a726acb8-e54f-44a9-b230-6c6cd5d4c783.PNG | PSNR: 34.1442 | SSIM: 0.9348\n",
      "Processed: 270_img_270_original_person643_bacteria_2534.jpeg_00079059-5012-4afe-8e01-b6cf1b2ecd7e.PNG | PSNR: 33.9066 | SSIM: 0.9152\n",
      "Processed: 266_img_266_original_person617_bacteria_2488.jpeg_ca698e58-130d-4493-ba1f-433895dabeb0.PNG | PSNR: 34.1442 | SSIM: 0.9348\n",
      "Processed: 268_img_268_original_person640_bacteria_2532.jpeg_b80c4941-8a48-4ad1-9595-bc5c0672b9b3.PNG | PSNR: 35.4465 | SSIM: 0.9467\n",
      "Processed: 275_img_275_original_person802_bacteria_2708.jpeg_020b7a28-b07f-4ee6-9bfc-7eb05dacee4c.PNG | PSNR: 33.2357 | SSIM: 0.9246\n",
      "Processed: 274_img_274_original_person781_virus_1412.jpeg_cde83fb6-c3d5-48b7-831b-5733c8e65073.PNG | PSNR: 32.6148 | SSIM: 0.9126\n",
      "Processed: 263_img_263_original_person550_bacteria_2309.jpeg_68c8647f-e889-4230-8eb4-6ee7894177d6.PNG | PSNR: 33.9369 | SSIM: 0.9328\n",
      "Processed: 275_img_275_original_person802_bacteria_2708.jpeg_0ec93838-c885-4804-9e86-65c1668e9d7b.PNG | PSNR: 33.3045 | SSIM: 0.9248\n",
      "Processed: 266_img_266_original_person617_bacteria_2488.jpeg_54f5d0ee-4988-4c31-bbc9-bea28f29710a.PNG | PSNR: 34.2297 | SSIM: 0.9330\n",
      "Processed: 267_img_267_original_person623_virus_1197.jpeg_836b184d-26b9-4693-9239-e9e584c9d011.PNG | PSNR: 32.3212 | SSIM: 0.9398\n",
      "Processed: 258_img_258_original_person496_bacteria_2095.jpeg_8686daea-4df4-4380-85b9-540920ab1055.PNG | PSNR: 33.4900 | SSIM: 0.9255\n",
      "Processed: 266_img_266_original_person617_bacteria_2488.jpeg_a8a8abd8-980f-4047-b2a2-391022283751.PNG | PSNR: 34.1442 | SSIM: 0.9348\n",
      "Processed: 269_img_269_original_person641_bacteria_2533.jpeg_61a61210-3203-448a-93dc-5fe136cc92fa.PNG | PSNR: 34.6750 | SSIM: 0.9437\n",
      "Processed: 258_img_258_original_person496_bacteria_2095.jpeg_c55e1329-b0ec-40b6-a14d-2a71d09cc5da.PNG | PSNR: 33.4900 | SSIM: 0.9255\n",
      "Processed: 271_img_271_original_person707_virus_1305.jpeg_434c9244-da34-4275-a3f7-eede924eed8e.PNG | PSNR: 34.5154 | SSIM: 0.9437\n",
      "Processed: 261_img_261_original_person527_virus_1048.jpeg_27e22b59-f712-40be-b20e-12835a37cf5f.PNG | PSNR: 34.1519 | SSIM: 0.9347\n",
      "Processed: 270_img_270_original_person643_bacteria_2534.jpeg_461ebafb-a407-4e92-86e5-fe8dd8f5f4f6.PNG | PSNR: 33.7798 | SSIM: 0.9158\n",
      "Processed: 265_img_265_original_person609_virus_1176.jpeg_e50d935d-a094-4398-b887-8bc595a602de.PNG | PSNR: 33.3786 | SSIM: 0.9283\n",
      "Processed: 265_img_265_original_person609_virus_1176.jpeg_70ba6ff7-e236-4d94-936d-5facccc04e88.PNG | PSNR: 33.0693 | SSIM: 0.9269\n",
      "Processed: 273_img_273_original_person776_virus_1405.jpeg_1fb5d6de-a36d-4014-b3e1-249cd8aea393.PNG | PSNR: 35.4194 | SSIM: 0.9523\n",
      "Processed: 267_img_267_original_person623_virus_1197.jpeg_510d1ee0-b6f0-4c31-bc27-b186ab9c9623.PNG | PSNR: 32.0273 | SSIM: 0.9405\n",
      "Processed: 267_img_267_original_person623_virus_1197.jpeg_afddd97a-64e5-486a-ac34-883a85c916a9.PNG | PSNR: 31.9574 | SSIM: 0.9402\n",
      "Processed: 257_img_257_original_person469_virus_965.jpeg_0aabea63-4d64-4b68-b306-c25fa77c8006.PNG | PSNR: 35.8902 | SSIM: 0.9033\n",
      "Processed: 262_img_262_original_person534_bacteria_2254.jpeg_683f80e9-cfbb-4a16-b0be-82e1fe9ff54f.PNG | PSNR: 34.2953 | SSIM: 0.9122\n",
      "Processed: 259_img_259_original_person498_bacteria_2100.jpeg_4ea16057-1426-424d-8cf5-b4a4be972ab4.PNG | PSNR: 33.9725 | SSIM: 0.9286\n",
      "Processed: 257_img_257_original_person469_virus_965.jpeg_5713af0d-0f22-42d9-8091-2a700457eeab.PNG | PSNR: 35.8294 | SSIM: 0.9030\n",
      "Processed: 274_img_274_original_person781_virus_1412.jpeg_74eac99f-730c-4545-85fa-db7becd99932.PNG | PSNR: 32.6148 | SSIM: 0.9126\n",
      "Processed: 260_img_260_original_person498_bacteria_2101.jpeg_8f5ca89c-c0fa-4f01-836d-66753023664a.PNG | PSNR: 34.3511 | SSIM: 0.9345\n",
      "Processed: 273_img_273_original_person776_virus_1405.jpeg_7f9f0116-6f31-474f-b3db-3bd5805292fe.PNG | PSNR: 35.7937 | SSIM: 0.9538\n",
      "Processed: 272_img_272_original_person762_virus_1390.jpeg_47635ca1-a638-4182-824c-d14f55ea24a5.PNG | PSNR: 35.8898 | SSIM: 0.9456\n",
      "Processed: 276_img_276_original_person820_bacteria_2731.jpeg_b2c9b9cc-7e4e-42db-85d8-14a173c5e664.PNG | PSNR: 35.9559 | SSIM: 0.9420\n",
      "Processed: 257_img_257_original_person469_virus_965.jpeg_7e8b33f2-abaf-414d-b885-4602df99952a.PNG | PSNR: 35.8902 | SSIM: 0.9033\n",
      "Processed: 260_img_260_original_person498_bacteria_2101.jpeg_3aedc9e3-cbda-4ca7-bcce-c6d2c6b7bb31.PNG | PSNR: 34.3825 | SSIM: 0.9336\n",
      "Processed: 262_img_262_original_person534_bacteria_2254.jpeg_41153bc9-fe60-48eb-a6cd-55bede628dd7.PNG | PSNR: 34.5445 | SSIM: 0.9112\n",
      "Processed: 271_img_271_original_person707_virus_1305.jpeg_56c07012-423c-46c4-82e5-452a6f5260e1.PNG | PSNR: 34.5154 | SSIM: 0.9437\n",
      "Processed: 262_img_262_original_person534_bacteria_2254.jpeg_7558ca04-674e-496b-93d1-4dbd8f82a876.PNG | PSNR: 34.2953 | SSIM: 0.9122\n",
      "Processed: 267_img_267_original_person623_virus_1197.jpeg_9fdcc3a4-2268-41e0-b0fc-b2c2a2f6fc90.PNG | PSNR: 32.3212 | SSIM: 0.9398\n",
      "Processed: 264_img_264_original_person593_bacteria_2435.jpeg_d57691ff-4f84-4f52-a3f8-424e115915e5.PNG | PSNR: 37.4689 | SSIM: 0.9508\n",
      "Processed: 261_img_261_original_person527_virus_1048.jpeg_887157b6-474d-4406-866a-b07d8971ee0a.PNG | PSNR: 33.7989 | SSIM: 0.9359\n",
      "Processed: 267_img_267_original_person623_virus_1197.jpeg_1f928d42-e50e-400a-9942-8cb496501d6a.PNG | PSNR: 32.3212 | SSIM: 0.9398\n",
      "Processed: 260_img_260_original_person498_bacteria_2101.jpeg_fb68f374-a130-4ca0-a31b-2279dd69b8e3.PNG | PSNR: 34.5884 | SSIM: 0.9336\n",
      "Processed: 265_img_265_original_person609_virus_1176.jpeg_235aea6b-9105-4166-995c-cc1e6e26da38.PNG | PSNR: 33.1987 | SSIM: 0.9258\n",
      "Processed: 268_img_268_original_person640_bacteria_2532.jpeg_854b3c43-893a-4b34-b3f7-73c69b552cc8.PNG | PSNR: 35.4465 | SSIM: 0.9467\n",
      "Processed: 275_img_275_original_person802_bacteria_2708.jpeg_aed8e200-47f3-4374-b6bb-52afaac0bdb8.PNG | PSNR: 33.6189 | SSIM: 0.9254\n",
      "Processed: 265_img_265_original_person609_virus_1176.jpeg_1f9d4257-2cfc-4336-86e4-069392d2e781.PNG | PSNR: 33.1987 | SSIM: 0.9258\n",
      "Processed: 273_img_273_original_person776_virus_1405.jpeg_050f5adb-dc3c-48f7-a3f6-acb6597985b3.PNG | PSNR: 35.7937 | SSIM: 0.9538\n",
      "Processed: 257_img_257_original_person469_virus_965.jpeg_6523454e-2a0f-4835-9277-25370e4123fe.PNG | PSNR: 35.8329 | SSIM: 0.9039\n",
      "Processed: 275_img_275_original_person802_bacteria_2708.jpeg_7d1d5254-bacf-459c-b862-1bc74e4b7ae4.PNG | PSNR: 33.6189 | SSIM: 0.9254\n",
      "Processed: 262_img_262_original_person534_bacteria_2254.jpeg_b99b7dbb-bd0a-4081-92a1-280617c595f9.PNG | PSNR: 34.1938 | SSIM: 0.9104\n",
      "Processed: 270_img_270_original_person643_bacteria_2534.jpeg_8f2a4476-c740-4a49-8d2b-a338857b5f67.PNG | PSNR: 33.7063 | SSIM: 0.9109\n",
      "Processed: 264_img_264_original_person593_bacteria_2435.jpeg_15424634-27ed-4e05-9d5b-0dda8c822f7b.PNG | PSNR: 37.4689 | SSIM: 0.9508\n",
      "Processed: 272_img_272_original_person762_virus_1390.jpeg_9d4155b3-657f-4993-a11d-0308fffa2e83.PNG | PSNR: 35.8898 | SSIM: 0.9456\n",
      "Processed: 273_img_273_original_person776_virus_1405.jpeg_1af4343e-9a20-48fb-bbd6-f49f6a49d099.PNG | PSNR: 35.4194 | SSIM: 0.9523\n",
      "Processed: 274_img_274_original_person781_virus_1412.jpeg_93a928f8-041d-40f3-8119-9ae51570a9e6.PNG | PSNR: 32.9020 | SSIM: 0.9158\n",
      "Processed: 259_img_259_original_person498_bacteria_2100.jpeg_e46ac722-c475-4c36-a3b1-038831287676.PNG | PSNR: 33.6950 | SSIM: 0.9295\n",
      "Processed: 268_img_268_original_person640_bacteria_2532.jpeg_50b3e8f0-0f4e-4ee8-9bba-dda965c71fe5.PNG | PSNR: 35.4465 | SSIM: 0.9467\n",
      "Processed: 263_img_263_original_person550_bacteria_2309.jpeg_3ddab90e-4f0b-4e03-b726-5cd14b66a5cb.PNG | PSNR: 33.9369 | SSIM: 0.9328\n",
      "Processed: 260_img_260_original_person498_bacteria_2101.jpeg_93dfd68b-13bb-46a4-8884-877afac579a8.PNG | PSNR: 34.3511 | SSIM: 0.9345\n",
      "Processed: 270_img_270_original_person643_bacteria_2534.jpeg_dc0780dc-e14a-4e72-8d58-166ae36e35df.PNG | PSNR: 33.7063 | SSIM: 0.9109\n",
      "Processed: 276_img_276_original_person820_bacteria_2731.jpeg_cde58807-46e0-40e4-8c75-f1cea7299201.PNG | PSNR: 35.6267 | SSIM: 0.9393\n",
      "Processed: 270_img_270_original_person643_bacteria_2534.jpeg_90d1f38d-bd32-4bf0-a4c5-d0f0152cd9d6.PNG | PSNR: 33.9066 | SSIM: 0.9152\n",
      "Processed: 263_img_263_original_person550_bacteria_2309.jpeg_ef506965-1a1c-44fc-af8f-00a211307d09.PNG | PSNR: 33.6538 | SSIM: 0.9327\n",
      "Processed: 264_img_264_original_person593_bacteria_2435.jpeg_3f569d57-6b32-4492-9a2f-3515af45ed6e.PNG | PSNR: 37.8318 | SSIM: 0.9511\n",
      "Processed: 258_img_258_original_person496_bacteria_2095.jpeg_68d44312-35fe-4936-a4e7-b8b467bd3690.PNG | PSNR: 33.4266 | SSIM: 0.9248\n",
      "Processed: 260_img_260_original_person498_bacteria_2101.jpeg_87d9f2a6-ea5f-4073-bf49-4167f64e73f7.PNG | PSNR: 34.3511 | SSIM: 0.9345\n",
      "Processed: 276_img_276_original_person820_bacteria_2731.jpeg_4c5fa249-2c0f-4ab3-a11b-7922d8ca4c3f.PNG | PSNR: 35.6978 | SSIM: 0.9408\n",
      "Processed: 269_img_269_original_person641_bacteria_2533.jpeg_72de6cdf-7522-458d-a0e0-3f15bd285c72.PNG | PSNR: 34.9118 | SSIM: 0.9432\n",
      "Processed: 259_img_259_original_person498_bacteria_2100.jpeg_ace243dc-b158-4331-bf53-711d3e894d66.PNG | PSNR: 33.9725 | SSIM: 0.9286\n",
      "Processed: 268_img_268_original_person640_bacteria_2532.jpeg_ac01c009-1984-4241-97e6-c1e194d6f921.PNG | PSNR: 35.7309 | SSIM: 0.9488\n",
      "Processed: 266_img_266_original_person617_bacteria_2488.jpeg_aae4ff67-3fed-4990-b7d9-4e3a065036e8.PNG | PSNR: 34.2421 | SSIM: 0.9334\n",
      "Processed: 262_img_262_original_person534_bacteria_2254.jpeg_f22d1897-8f74-4c7f-91ae-d397aaf15e12.PNG | PSNR: 34.1938 | SSIM: 0.9104\n",
      "Processed: 258_img_258_original_person496_bacteria_2095.jpeg_6da4d4a1-f01b-44da-9147-d3c3d6045fa3.PNG | PSNR: 33.4900 | SSIM: 0.9255\n",
      "Processed: 276_img_276_original_person820_bacteria_2731.jpeg_eb5d1833-69a9-414f-97ba-d48b49bb7c08.PNG | PSNR: 35.9559 | SSIM: 0.9420\n",
      "Processed: 269_img_269_original_person641_bacteria_2533.jpeg_4fb49dea-c83d-4096-b3fe-d1a946205914.PNG | PSNR: 34.9118 | SSIM: 0.9432\n",
      "Processed: 271_img_271_original_person707_virus_1305.jpeg_78569327-6299-4026-8a2d-16e8183d0d03.PNG | PSNR: 34.6335 | SSIM: 0.9437\n",
      "Processed: 261_img_261_original_person527_virus_1048.jpeg_ab5b9d2c-a5d9-49dc-8b72-09dde3c6850b.PNG | PSNR: 34.1519 | SSIM: 0.9347\n",
      "Processed: 263_img_263_original_person550_bacteria_2309.jpeg_8963b143-e4ce-43aa-a4d2-91f80f432503.PNG | PSNR: 33.7376 | SSIM: 0.9317\n",
      "Processed: 271_img_271_original_person707_virus_1305.jpeg_a975286d-53c7-4565-8ac1-336565089e7d.PNG | PSNR: 34.6335 | SSIM: 0.9437\n",
      "\n",
      "Test Completed\n",
      "Average PSNR: 34.4524\n",
      "Average SSIM: 0.9324\n",
      "\n",
      "Results:\n",
      "Average PSNR: 34.4524\n",
      "Average SSIM: 0.9324\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_msssim import ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as calculate_psnr\n",
    "from PIL import Image\n",
    "from vision_transformer import CSwinUnet\n",
    "\n",
    "class TestArgs:\n",
    "    def __init__(self):\n",
    "        self.test_blurred_dir = '/kaggle/input/ch-xrays-test/blurredd/blurredd'\n",
    "        self.test_sharp_dir = '/kaggle/input/ch-xrays-test/sharpp/sharpp'\n",
    "        self.generated_dir = '/kaggle/working/generated'\n",
    "        self.comparison_dir = '/kaggle/working/comparison'  # Add this line for comparison folder\n",
    "        self.model_path = '/kaggle/working/model_checkpoints/final_model.pth'\n",
    "        self.img_size = 224\n",
    "        self.batch_size = 1\n",
    "\n",
    "# Test dataset\n",
    "class TestXrayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, blurred_dir, sharp_dir, transform=None):\n",
    "        self.blurred_dir = blurred_dir\n",
    "        self.sharp_dir = sharp_dir\n",
    "        self.image_paths = [os.path.join(blurred_dir, f) for f in os.listdir(blurred_dir) if f.endswith(('.PNG', '.jpg', '.jpeg'))]  # .PNG extension handled\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        blurred_img_path = self.image_paths[idx]\n",
    "        sharp_img_path = os.path.join(self.sharp_dir, os.path.basename(blurred_img_path))\n",
    "        \n",
    "        blurred_img = Image.open(blurred_img_path).convert('L')  # Convert to grayscale\n",
    "        sharp_img = Image.open(sharp_img_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            blurred_img = self.transform(blurred_img)\n",
    "            sharp_img = self.transform(sharp_img)\n",
    "\n",
    "        return blurred_img, sharp_img, blurred_img_path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_model(args, model, dataloader, device):\n",
    "    model.eval()\n",
    "    os.makedirs(args.generated_dir, exist_ok=True)\n",
    "    os.makedirs(args.comparison_dir, exist_ok=True)  # Create comparison folder\n",
    "\n",
    "    psnr_values = []\n",
    "    ssim_values = []\n",
    "\n",
    "    transform_to_pil = transforms.ToPILImage()\n",
    "\n",
    "    for blurred, sharp, img_path in dataloader:\n",
    "        blurred, sharp = blurred.to(device), sharp.to(device)\n",
    "        \n",
    "        # Get the deblurred output\n",
    "        deblurred = model(blurred)\n",
    "        deblurred = torch.clamp(deblurred, 0, 1)  # Ensure values are in [0, 1]\n",
    "\n",
    "        # Save the deblurred image\n",
    "        output_image = transform_to_pil(deblurred.squeeze(0).cpu())\n",
    "        output_path = os.path.join(args.generated_dir, os.path.basename(img_path[0]))\n",
    "        output_image.save(output_path)\n",
    "\n",
    "        # Load the blurred and ground truth images to prepare the comparison image\n",
    "        blurred_image = transform_to_pil(blurred.squeeze(0).cpu())\n",
    "        sharp_image = transform_to_pil(sharp.squeeze(0).cpu())\n",
    "\n",
    "        # Create a figure for comparison\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "        # Plot the images side by side\n",
    "        ax[0].imshow(blurred_image, cmap='gray')\n",
    "        ax[0].set_title('Blurred')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        ax[1].imshow(output_image, cmap='gray')\n",
    "        ax[1].set_title('Generated')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        ax[2].imshow(sharp_image, cmap='gray')\n",
    "        ax[2].set_title('Ground Truth')\n",
    "        ax[2].axis('off')\n",
    "\n",
    "        # Save the comparison image\n",
    "        comparison_image_path = os.path.join(args.comparison_dir, os.path.basename(img_path[0]).replace(\".PNG\", \"_comparison.png\"))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_image_path)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Calculate PSNR and SSIM\n",
    "        sharp_np = sharp.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "        deblurred_np = deblurred.squeeze(0).cpu().numpy().transpose(1, 2, 0)  # Convert to HxWxC\n",
    "\n",
    "        psnr_value = calculate_psnr(sharp_np, deblurred_np, data_range=1.0)\n",
    "        ssim_value = ssim(torch.tensor(sharp_np).unsqueeze(0).unsqueeze(0), torch.tensor(deblurred_np).unsqueeze(0).unsqueeze(0), data_range=1.0, size_average=True).item()\n",
    "\n",
    "        psnr_values.append(psnr_value)\n",
    "        ssim_values.append(ssim_value)\n",
    "\n",
    "        print(f\"Processed: {os.path.basename(img_path[0])} | PSNR: {psnr_value:.4f} | SSIM: {ssim_value:.4f}\")\n",
    "\n",
    "    # Calculate and print average PSNR and SSIM\n",
    "    avg_psnr = np.mean(psnr_values)\n",
    "    avg_ssim = np.mean(ssim_values)\n",
    "\n",
    "    print(\"\\nTest Completed\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "    return avg_psnr, avg_ssim\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = TestArgs()\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the model\n",
    "    model = CSwinUnet(img_size=args.img_size, num_classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "    print(\"Model loaded successfully\")\n",
    "\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((args.img_size, args.img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Initialize dataset and dataloader\n",
    "    test_dataset = TestXrayDataset(\n",
    "        blurred_dir=args.test_blurred_dir,\n",
    "        sharp_dir=args.test_sharp_dir,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"Testing on {len(test_dataset)} images\")\n",
    "\n",
    "    # Test the model\n",
    "    avg_psnr, avg_ssim = test_model(args, model, test_dataloader, device)\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"Average PSNR: {avg_psnr:.4f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T18:19:40.922131Z",
     "iopub.status.busy": "2025-01-22T18:19:40.921774Z",
     "iopub.status.idle": "2025-01-22T18:19:40.986139Z",
     "shell.execute_reply": "2025-01-22T18:19:40.985447Z",
     "shell.execute_reply.started": "2025-01-22T18:19:40.922105Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/generated.zip'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to the generated folder\n",
    "generated_dir = '/kaggle/working/generated'\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(generated_dir, 'zip', generated_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-22T18:34:07.466557Z",
     "iopub.status.busy": "2025-01-22T18:34:07.466195Z",
     "iopub.status.idle": "2025-01-22T18:34:09.607318Z",
     "shell.execute_reply": "2025-01-22T18:34:09.606598Z",
     "shell.execute_reply.started": "2025-01-22T18:34:07.466528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/comparison.zip'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Path to the generated folder\n",
    "generated_dir = '/kaggle/working/comparison'\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(generated_dir, 'zip', generated_dir)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6507764,
     "sourceId": 10513530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6525100,
     "sourceId": 10546024,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
